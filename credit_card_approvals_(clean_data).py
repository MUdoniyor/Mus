# -*- coding: utf-8 -*-
"""Credit Card Approvals (Clean Data).ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1cyZLmrAa-VYu1MTKzKUh9akuZg8kF8WL
"""

# Connect google drive
from google.colab import drive
drive.mount("/content/drive")

# Commented out IPython magic to ensure Python compatibility.
# Import Required Packages for EDA

# os library is used to interact with the operating system, such as navigating file directories
import os

# pandas library is used for data manipulation and analysis, providing data structures such as DataFrames and Series
import pandas as pd

# numpy library is used for numerical operations in Python, providing a variety of mathematical functions that are useful for working with arrays of numbers
import numpy as np

# matplotlib library is used for creating visualizations, providing a wide variety of plotting functions and styles to create static, interactive, and animated plots
import matplotlib.pyplot as plt

# seaborn library provides a higher-level interface for creating statistical graphics, simplifying the process of creating common types of plots, such as heatmaps, scatterplots, and bar charts
import seaborn as sns

# missingno library is used for visualizing missing data in a dataset, providing a simple way to visualize the distribution of missing values in a DataFrame
import missingno as msno 

# plotly.graph_objects library is used for creating interactive visualizations, providing a variety of chart types and features for creating highly customizable and interactive graphs
import plotly.graph_objects as go 

# plotly.express library provides a simpler interface for creating many types of charts, including scatterplots, line charts, and bar charts, built on top of plotly.graph_objects
import plotly.express as px 

# %matplotlib inline magic command is used to display plots inline in Jupyter notebooks
# %matplotlib inline

# warnings library is used to handle warning messages in Python, allowing you to control the behavior of warnings in your code
import warnings

# warnings.filterwarnings() method allows you to control the behavior of warnings in your code, such as ignoring certain warnings or displaying them only once
warnings.filterwarnings('ignore')

#Read the dataset/s
# df - field, which is going to read all the content from the file and it will be stored into df
# pd  - Panda Library (reads everything from the file)
# pd.read_csv() method is used to read a CSV file and create a DataFrame from it
# The file path to the CSV file is specified as a string, in this case, '/content/drive/MyDrive/Capstone Project 3/clean_dataset.csv'
# The resulting DataFrame is stored in a variable called 'df', which can be used to access and manipulate the data in the CSV file
df = pd.read_csv('/content/drive/MyDrive/Capstone Project 3/clean_dataset.csv')

#1 Checking description(first 5 and last 5 rows)
# The .head() method is used to display the first few rows of a DataFrame, by default the first 5 rows
# This can be useful for quickly checking the structure and contents of a DataFrame, to ensure that the data has been loaded and processed correctly
df.head()

df.tail() #last 5 rows

#Check how many total number of rows and columns-data shape(attributes & samples)
# The .shape attribute is used to retrieve the dimensions of a DataFrame as a tuple, where the first element is the number of rows and the second element is the number of columns
# This can be useful for quickly checking the size of a DataFrame, especially when working with large datasets

df.shape

# name of the attributes
df.columns

#unique values for each attribute (Useful for predictive analysis)
# The .nunique() method is used to count the number of unique values in each column of a DataFrame
# This can be useful for getting a quick summary of the data in each column, to understand the range of values and the presence of any duplicates or missing values

df.nunique()

#Complete info about data frame
# The .info() method is used to display a summary of the DataFrame, including the column names, data types, non-null values, and memory usage
# This can be useful for getting a quick overview of the dataset, including any missing or unexpected data types, and the total memory usage of the DataFrame

df.info()

# Visualising data  distribution in detail
#  plt - library (import matplotlib.pyplot as plt)

# Creates a new Figure object using the plt.figure() method, and sets the size of the figure to (16,16)
# using the figsize parameter. This is the size of the plot in inches.
fig = plt.figure(figsize =(16,16))
# Gets the current axes of the Figure object, using the gca() method of the Figure object.
ax=fig.gca()
# Creates a histogram # of the data in the DataFrame, using the hist() method.
df.hist(ax=ax,bins =30)
# Displays the plot using the plt.show() method, which shows the plot in a new window.
# The ax parameter specifies the axes to use for the plot, which in this case is the current 
# axes of the Figure object. The bins parameter sets the number of bins to use for the histogram.
plt.show()
# The resulting plot is a histogram of the values in each column of the DataFrame, 
# which can be useful for visualizing the distribution of the data 
# and identifying any outliers or unexpected patterns.

#detecting outliers
df.plot(kind='box', subplots=True, 
        layout=(2,7),sharex=False,sharey=False, figsize=(20, 10), color='deeppink');

#identify the outliers
# define continuous variable & plot
continous_features = ['Age','Debt','YearsEmployed', 'CreditScore','ZipCode', 'Income']  
def outliers(df_out, drop = False):
    for each_feature in df_out.columns:
        feature_data = df_out[each_feature]
        Q1 = np.percentile(feature_data, 25.) # 25th percentile of the data of the given feature
        Q3 = np.percentile(feature_data, 75.) # 75th percentile of the data of the given feature
        IQR = Q3-Q1 #Interquartile Range
        outlier_step = IQR * 1.5 #That's we were talking about above
        outliers = feature_data[~((feature_data >= Q1 - outlier_step) & (feature_data <= Q3 + outlier_step))].index.tolist()  
        if not drop:
            print('For the feature {}, No of Outliers is {}'.format(each_feature, len(outliers)))
        if drop:
            df.drop(outliers, inplace = True, errors = 'ignore')
            print('Outliers from {} feature removed'.format(each_feature))
outliers(df[continous_features])

#drop the outliers
outliers(df[continous_features], drop = True)

#check if outliers got removed
df.plot(kind='box', subplots=True, 
        layout=(2,7),sharex=False,sharey=False, figsize=(20, 10), color='deeppink');

#Check data shape after outlier removal
df.shape

#checking target value distribution
print(df.Approved.value_counts())
fig, ax = plt.subplots(figsize=(5,4))
name = ["Approved", "Not_Approved"]
ax = df.Approved.value_counts().plot(kind='bar')
ax.set_title("Credit Card Approval Classes", fontsize = 13, weight = 'bold')
ax.set_xticklabels (name, rotation = 0)

# To calculate the percentage
totals = []
for i in ax.patches:
    totals.append(i.get_height())
total = sum(totals)
for i in ax.patches:
    ax.text(i.get_x()+.09, i.get_height()-50, \
            str(round((i.get_height()/total)*100, 2))+'%', fontsize=14,
                color='white', weight = 'bold')
    
plt.tight_layout()

#check correlation between variables
sns.set(style="white") 
plt.rcParams['figure.figsize'] = (15, 10) 
sns.heatmap(df.corr(), annot = True, linewidths=.5, cmap="Blues")
plt.title('Corelation Between Variables', fontsize = 30)
plt.show()

!pip install https://github.com/pandas-profiling/pandas-profiling/archive/master.zip

#obtain full profiler report
#restart kernel
#re-run import libraries and data
import pandas as pd
import numpy as np
from pandas_profiling import ProfileReport
profile = ProfileReport(df,title="Credit Card Approvals",
                        html={'style':{'full_width':True}})
profile.to_notebook_iframe()

#pre-processing
from sklearn.exceptions import DataDimensionalityWarning
#encode object columns to integers
from sklearn import preprocessing
from sklearn.preprocessing import OrdinalEncoder

for col in df:
  if df[col].dtype =='object':
    df[col]=OrdinalEncoder().fit_transform(df[col].values.reshape(-1,1))
df

class_label =df['Approved']
df = df.drop(['Approved'], axis =1)
df = (df-df.min())/(df.max()-df.min())
df['Approved']=class_label
df
# Overall, this code is used to perform data preprocessing 
# by normalizing the data and separating the target variable (i.e., 'Approved') from the feature set.

#pre-processing
cc_data = df.copy()
le = preprocessing.LabelEncoder()
gender = le.fit_transform(list(cc_data["Gender"])) # gender (1 = male; 0 = female)
age = le.fit_transform(list(cc_data["Age"])) # age in years
debt = le.fit_transform(list(cc_data["Debt"])) # debt amount
married = le.fit_transform(list(cc_data["Married"])) # marital status (1 = yes; 0 = no)
bankcustomer = le.fit_transform(list(cc_data["BankCustomer"])) # bank customer (1 = yes; 0 = no)
industry = le.fit_transform(list(cc_data["Industry"])) # type of industry
ethnicity = le.fit_transform(list(cc_data["Ethnicity"])) # ethnicity
yearsemployed = le.fit_transform(list(cc_data["YearsEmployed"])) # years employed
priordefault = le.fit_transform(list(cc_data["PriorDefault"])) # previous credit default (1 = yes; 0 = no)
employed = le.fit_transform(list(cc_data["Employed"])) # employed status (1 = yes; 0 = no)
creditscore = le.fit_transform(list(cc_data["CreditScore"])) # credit score
driverslicense = le.fit_transform(list(cc_data["DriversLicense"])) # driver's license (1 = yes; 0 = no)
citizen = le.fit_transform(list(cc_data["Citizen"])) # citizenship status
zipcode = le.fit_transform(list(cc_data["ZipCode"])) # zip code
income = le.fit_transform(list(cc_data["Income"])) # income
approved = le.fit_transform(list(cc_data["Approved"])) # credit card approval (1 = approved; 0 = not approved)

import sklearn.model_selection
x = list(zip(gender,	age,	debt,	married,	bankcustomer,	industry,	ethnicity,	yearsemployed,	priordefault,	employed,	creditscore,	driverslicense,	citizen,	zipcode,	income))
y = list(approved)
# Test options and evaluation metric
num_folds = 5
seed = 7
scoring = 'accuracy'

# Model Test/Train
# Splitting what we are trying to predict into 4 different arrays -
# X train is a section of the x array(attributes) and vise versa for Y(features)
# The test data will test the accuracy of the model created
x_train, x_test, y_train, y_test = sklearn.model_selection.train_test_split(x, y, test_size = 0.20, random_state=seed)
#splitting 20% of our data into test samples. If we train the model with higher data it already has seen that information and knows

#size of train and test subsets after splitting
np.shape(x_train), np.shape(x_test)

# Predictive analytics model development by comparing different Scikit-learn classification algorithms
from sklearn.preprocessing import StandardScaler
from sklearn.model_selection import train_test_split
from sklearn.model_selection import KFold
from sklearn.model_selection import cross_val_score
from sklearn.model_selection import GridSearchCV
from sklearn.metrics import classification_report
from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay
from sklearn.metrics import accuracy_score
from sklearn.pipeline import Pipeline
from sklearn.linear_model import LogisticRegression
from sklearn.tree import DecisionTreeClassifier
from sklearn.neighbors import KNeighborsClassifier
from sklearn.discriminant_analysis import LinearDiscriminantAnalysis
from sklearn.naive_bayes import GaussianNB
from sklearn.svm import SVC
from sklearn.ensemble import AdaBoostClassifier
from sklearn.ensemble import GradientBoostingClassifier
from sklearn.ensemble import RandomForestClassifier
from sklearn.ensemble import ExtraTreesClassifier

models = []
models.append(('NB', GaussianNB()))
models.append(('SVM', SVC()))
models.append(('GBM', GradientBoostingClassifier()))
models.append(('RF', RandomForestClassifier()))
# evaluate each model in turn
results = []
names = []
print("Performance on Training set")
for name, model in models:
  kfold = KFold(n_splits=num_folds,shuffle=True,random_state=seed)
  cv_results = cross_val_score(model, x_train, y_train, cv=kfold, scoring='accuracy')
  results.append(cv_results)
  names.append(name)
  msg = "%s: %f (%f)" % (name, cv_results.mean(), cv_results.std())
  msg += '\n'
  print(msg)

# Compare Algorithms' Performance
fig = plt.figure()
fig.suptitle('Algorithm Comparison')
ax = fig.add_subplot(111)
plt.boxplot(results)
ax.set_xticklabels(names)
plt.show()

#Model Evaluation by testing with independent/external test data set. 
# Make predictions on validation/test dataset

#Model Evaluation by testing with independent/external test data set. 
# Make predictions on validation/test dataset

models.append(('DT', DecisionTreeClassifier()))
models.append(('NB', GaussianNB()))
models.append(('SVM', SVC()))
models.append(('GBM', GradientBoostingClassifier()))
models.append(('RF', RandomForestClassifier()))
dt = DecisionTreeClassifier()
nb = GaussianNB()
gb = GradientBoostingClassifier()
rf = RandomForestClassifier()

best_model = rf
best_model.fit(x_train, y_train)
y_pred = best_model.predict(x_test)
print("Best Model Accuracy Score on Test Set:", accuracy_score(y_test, y_pred))

#Model Performance Evaluation Metric 1 - Classification Report
print(classification_report(y_test, y_pred))

#Model Performance Evaluation Metric 2
#Confusion matrix 
from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay
cm = confusion_matrix(y_test, y_pred)
disp = ConfusionMatrixDisplay(confusion_matrix=cm)
disp.plot()
plt.show()

#Model Evaluation Metric 3- ROC-AUC curve
from sklearn.metrics import roc_auc_score
from sklearn.metrics import roc_curve

best_model = rf
best_model.fit(x_train, y_train)
rf_roc_auc = roc_auc_score(y_test,best_model.predict(x_test))
fpr,tpr,thresholds = roc_curve(y_test, best_model.predict_proba(x_test)[:,1])

plt.figure()
plt.plot(fpr,tpr,label = 'Random Forest(area = %0.2f)'% rf_roc_auc)
plt.plot([0,1],[0,1],'r--')
plt.xlim([0.0,1.0])
plt.ylim([0.0,1.05])
plt.xlabel('False positive rate')
plt.ylabel('True positive rate')
plt.title('Receiver Operating Characteristic')
plt.legend(loc='lower right')
plt.savefig('LOC_ROC')
plt.show()

#Model Evaluation Metric 4-prediction report
for x in range(len(y_pred)):
  print("Predicted: ", y_pred[x], "Actual: ", y_test[x], "Data: ", x_test[x],)